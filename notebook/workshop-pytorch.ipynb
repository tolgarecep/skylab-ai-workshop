{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4466377",
   "metadata": {},
   "source": [
    "<h2>PyTorch Implementation of the Workshop Notebook</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99bfab",
   "metadata": {},
   "source": [
    "PyTorch alternative for the classification with deep learning section of workshop notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1da4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf22c11",
   "metadata": {},
   "source": [
    "<h3>Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b6ce8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = r'..\\dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f015d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images belonging to class \" crazing \": 180\n",
      "Images belonging to class \" inclusion \": 180\n",
      "Images belonging to class \" patches \": 180\n",
      "Images belonging to class \" pitted_surface \": 180\n",
      "Images belonging to class \" rolled-in_scale \": 180\n",
      "Images belonging to class \" scratches \": 180\n",
      "We have  1080  images\n"
     ]
    }
   ],
   "source": [
    "categories = os.listdir(images_dir)\n",
    "m = 0\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(images_dir, category)\n",
    "    class_size = len(os.listdir(category_dir))\n",
    "    print('Images belonging to class \"', category, '\":', class_size)\n",
    "    m += class_size\n",
    "print('We have ', m, ' images') # 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c29ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'..\\train'\n",
    "val_dir = r'..\\val'\n",
    "os.mkdir(val_dir)\n",
    "test_dir = r'..\\test'\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b2c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n",
      "6 classes\n"
     ]
    }
   ],
   "source": [
    "print(categories)\n",
    "print(len(categories), 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b993e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    path = os.path.join(val_dir, category)\n",
    "    os.mkdir(path)\n",
    "    path = os.path.join(test_dir, category)\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842be0dc",
   "metadata": {},
   "source": [
    "Splitting 20% of total to validation set and another 20% for the test set, as done in original notebook. Since we have 6 classes, and we want to split this 20 percents without creating imbalances in test and validation sets, we take `(m*.2)/6`-image from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b81775",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int((m*.2)//6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc4cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in categories:\n",
    "    src_dir = os.path.join(images_dir, class_name)\n",
    "    imgs_to_relocate = os.listdir(src_dir)[:split]\n",
    "    for image in imgs_to_relocate:\n",
    "        src = os.path.join(src_dir, image)\n",
    "        dst = os.path.join(test_dir, class_name, image)\n",
    "        shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24878719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in categories:\n",
    "    src_dir = os.path.join(images_dir, class_name)\n",
    "    imgs_to_relocate = os.listdir(src_dir)[:split]\n",
    "    for image in imgs_to_relocate:\n",
    "        src = os.path.join(src_dir, image)\n",
    "        dst = os.path.join(val_dir, class_name, image)\n",
    "        shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b37420",
   "metadata": {},
   "source": [
    "Checking sample sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a16931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test set, images belonging to class \" crazing \": 60\n",
      "In test set, images belonging to class \" inclusion \": 60\n",
      "In test set, images belonging to class \" patches \": 60\n",
      "In test set, images belonging to class \" pitted_surface \": 60\n",
      "In test set, images belonging to class \" rolled-in_scale \": 60\n",
      "In test set, images belonging to class \" scratches \": 60\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    category_dir = os.path.join(test_dir, category)\n",
    "    class_size = len(os.listdir(category_dir))\n",
    "    print('In test set, images belonging to class \"', category, '\":', class_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f64f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In validation set, images belonging to class \" crazing \": 60\n",
      "In validation set, images belonging to class \" inclusion \": 60\n",
      "In validation set, images belonging to class \" patches \": 60\n",
      "In validation set, images belonging to class \" pitted_surface \": 60\n",
      "In validation set, images belonging to class \" rolled-in_scale \": 60\n",
      "In validation set, images belonging to class \" scratches \": 60\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    category_dir = os.path.join(val_dir, category)\n",
    "    class_size = len(os.listdir(category_dir))\n",
    "    print('In validation set, images belonging to class \"', category, '\":', class_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b8089",
   "metadata": {},
   "source": [
    "Now decreased dataset file contains our training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f04513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training set, images belonging to class \" crazing \": 180\n",
      "In training set, images belonging to class \" inclusion \": 180\n",
      "In training set, images belonging to class \" patches \": 180\n",
      "In training set, images belonging to class \" pitted_surface \": 180\n",
      "In training set, images belonging to class \" rolled-in_scale \": 180\n",
      "In training set, images belonging to class \" scratches \": 180\n"
     ]
    }
   ],
   "source": [
    "train_dir = r'..\\dataset'\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(train_dir, category)\n",
    "    class_size = len(os.listdir(category_dir))\n",
    "    print('In training set, images belonging to class \"', category, '\":', class_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445147f2",
   "metadata": {},
   "source": [
    "Image examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e2818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename: str=None) -> None:\n",
    "    \"\"\"\n",
    "    View multiple images stored in files, stacking vertically\n",
    "\n",
    "    Arguments:\n",
    "        filename: str - path to filename containing image\n",
    "    \"\"\"\n",
    "    image = mpimg.imread(filename)\n",
    "    # <something gets done here>\n",
    "    plt.figure()\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8eb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_images_list = [r'..\\test\\scratches\\scratches_1.jpg',\n",
    "#                       r'..\\test\\scratches\\inclusion_1.jpg',\n",
    "#                       r'..\\test\\scratches\\patches_1.jpg']\n",
    "\n",
    "# for image in sample_images_list:\n",
    "#     process(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bfe0c",
   "metadata": {},
   "source": [
    "NOTE: Dead kernel error after running Matplotlib is a current problem: https://stackoverflow.com/questions/69786885/after-conda-update-python-kernel-crashes-when-matplotlib-is-used. You can take a look at the samples from the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9822c8a",
   "metadata": {},
   "source": [
    "<h3>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb2929",
   "metadata": {},
   "source": [
    "`transforms.ToTensor()` transforms image to tensor with floats as well as rescaling pixel values between 0 and 255 to 0 and 1 (by simply dividing tensors with 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad4946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.Resize(100), # image size is 100 x 100 as determined in original notebook\n",
    "                                 transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                             std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c798ced",
   "metadata": {},
   "source": [
    "If you want to normalize images, PyTorch needs mean and standard deviation of each channel of images as parameters for `transforms.Normalize()`. Instead of computing the values specific for your dataset, you can use values of ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd4edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.ImageFolder(root=train_dir, transform=transforms)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_dir, transform=transforms)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_dir, transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e8f73",
   "metadata": {},
   "source": [
    "Finally, we'll create `DataLoader` instances that are appropriate type of input for PyTorch neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816d5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # as determined in original notebook\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea57e4e",
   "metadata": {},
   "source": [
    "<h3>Modeling</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ece77",
   "metadata": {},
   "source": [
    "Some remarks:\n",
    "1. TensorFlow's `layers.MaxPooling2D()` has `pool_size=(2,2)` as default, while PyTorch has no default value for `kernel_size` (they determine the same thing).\n",
    "2. Implementation of a convolutional layer: In TF, arguments are `(filters, kernel_size, ...)`; in PyTorch, it's `(in_channels, out_channels, ...)`.\n",
    "3. A convention difference between these two libraries is about the order of dimensions: In TF, input tensor is of shape `(batch_dim, height, width, channels)` while in PyTorch the shape is `(batch_dim, channels, height, width)`.\n",
    "4. `torch.nn.CrossEntropyLoss()` utilizes the softmax function before computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fb8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding='same'), nn.ReLU(),\n",
    "                                      nn.MaxPool2d(kernel_size=2),\n",
    "                                      \n",
    "                                      nn.Conv2d(16, 32, kernel_size=3, padding='same'), nn.ReLU(),\n",
    "                                      nn.MaxPool2d(kernel_size=2),\n",
    "                                      \n",
    "                                      nn.Conv2d(32, 64, kernel_size=3, padding='same'), nn.ReLU(),\n",
    "                                      nn.MaxPool2d(kernel_size=2))\n",
    "        self.classifier = nn.Sequential(nn.Linear(9216, 128), nn.ReLU(),\n",
    "                                        nn.Linear(128, 6)) # classification layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1) # flattening the output of convolution part of the network\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050eb54",
   "metadata": {},
   "source": [
    "Now we define a function to do training and computing loss (model will be validated on validation data at the end of each epoch, which is a process also visible in TF's history object's print.) Before all, in order to utilize GPU, we have to define an object like below and save models and tensors to the GPU whenever we're going to work with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7eae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591c05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn0 = CNNNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f2332",
   "metadata": {},
   "source": [
    "Setting the optimizer (will pass a different value to learning rate because original one results in flat learning curve.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da9b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer0 = optim.Adam(cnn0.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "345ed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20):\n",
    "    # training\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader: # for a batch\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device) # utilizing the GPU\n",
    "            output = model(inputs) # obtaining outputs of the network\n",
    "            loss = loss_fn(output, targets) # computing loss\n",
    "            loss.backward() # computing the gradients\n",
    "            optimizer.step() # using gradients for optimization of parameters\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(train_loader) # computing training loss at the end of an epoch\n",
    "    \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # validation\n",
    "        for inputs, targets in val_loader: # for a batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output, targets)\n",
    "            valid_loss += loss.data.item()\n",
    "            # computing accuracy\n",
    "            _, y_pred_tags = output.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += y_pred_tags.eq(targets).sum().item()\n",
    "            acc = 100.*correct/total\n",
    "        valid_loss /= len(val_loader)\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, Accuracy: {:.2f}'.format(epoch, training_loss,\n",
    "                                                                                                   valid_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b8ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 3.09, Validation Loss: 1.88, Accuracy: 23.89\n",
      "Epoch: 1, Training Loss: 1.81, Validation Loss: 4.84, Accuracy: 16.67\n",
      "Epoch: 2, Training Loss: 2.06, Validation Loss: 1.80, Accuracy: 16.67\n",
      "Epoch: 3, Training Loss: 1.82, Validation Loss: 1.80, Accuracy: 16.67\n",
      "Epoch: 4, Training Loss: 1.84, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 5, Training Loss: 1.81, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 6, Training Loss: 1.81, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 7, Training Loss: 1.81, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 8, Training Loss: 1.80, Validation Loss: 1.82, Accuracy: 16.67\n",
      "Epoch: 9, Training Loss: 1.75, Validation Loss: 1.80, Accuracy: 16.67\n",
      "Epoch: 10, Training Loss: 2.01, Validation Loss: 10.93, Accuracy: 16.67\n",
      "Epoch: 11, Training Loss: 2.01, Validation Loss: 21.31, Accuracy: 16.67\n",
      "Epoch: 12, Training Loss: 1.86, Validation Loss: 1.94, Accuracy: 16.67\n",
      "Epoch: 13, Training Loss: 1.83, Validation Loss: 26.89, Accuracy: 16.67\n",
      "Epoch: 14, Training Loss: 2.18, Validation Loss: 10.03, Accuracy: 16.67\n",
      "Epoch: 15, Training Loss: 2.03, Validation Loss: 1.80, Accuracy: 16.67\n",
      "Epoch: 16, Training Loss: 1.85, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 17, Training Loss: 1.80, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 18, Training Loss: 1.80, Validation Loss: 1.79, Accuracy: 16.67\n",
      "Epoch: 19, Training Loss: 1.80, Validation Loss: 1.79, Accuracy: 16.67\n"
     ]
    }
   ],
   "source": [
    "train(cnn0, optimizer0, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c8f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = CNNNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b125e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.Adam(cnn1.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0173f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.89, Validation Loss: 1.76, Accuracy: 27.22\n",
      "Epoch: 1, Training Loss: 1.78, Validation Loss: 1.70, Accuracy: 30.28\n",
      "Epoch: 2, Training Loss: 1.76, Validation Loss: 1.62, Accuracy: 27.22\n",
      "Epoch: 3, Training Loss: 1.72, Validation Loss: 1.57, Accuracy: 25.83\n",
      "Epoch: 4, Training Loss: 1.74, Validation Loss: 1.55, Accuracy: 43.89\n",
      "Epoch: 5, Training Loss: 1.70, Validation Loss: 1.50, Accuracy: 40.56\n",
      "Epoch: 6, Training Loss: 1.69, Validation Loss: 1.44, Accuracy: 45.00\n",
      "Epoch: 7, Training Loss: 1.63, Validation Loss: 1.40, Accuracy: 48.33\n",
      "Epoch: 8, Training Loss: 1.61, Validation Loss: 1.35, Accuracy: 50.28\n",
      "Epoch: 9, Training Loss: 1.57, Validation Loss: 1.30, Accuracy: 53.33\n",
      "Epoch: 10, Training Loss: 1.54, Validation Loss: 1.25, Accuracy: 58.33\n",
      "Epoch: 11, Training Loss: 1.49, Validation Loss: 1.19, Accuracy: 61.94\n",
      "Epoch: 12, Training Loss: 1.44, Validation Loss: 1.10, Accuracy: 67.50\n",
      "Epoch: 13, Training Loss: 1.35, Validation Loss: 1.02, Accuracy: 70.56\n",
      "Epoch: 14, Training Loss: 1.27, Validation Loss: 0.94, Accuracy: 74.17\n",
      "Epoch: 15, Training Loss: 1.20, Validation Loss: 0.90, Accuracy: 74.44\n",
      "Epoch: 16, Training Loss: 1.13, Validation Loss: 0.85, Accuracy: 77.22\n",
      "Epoch: 17, Training Loss: 1.06, Validation Loss: 0.80, Accuracy: 78.61\n",
      "Epoch: 18, Training Loss: 1.01, Validation Loss: 0.75, Accuracy: 81.39\n",
      "Epoch: 19, Training Loss: 0.95, Validation Loss: 0.71, Accuracy: 82.50\n"
     ]
    }
   ],
   "source": [
    "train(cnn1, optimizer1, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df684c89",
   "metadata": {},
   "source": [
    "As we're underfitting, we should increase number of epochs and train again. We can continue from where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6559eac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.90, Validation Loss: 0.67, Accuracy: 83.89\n",
      "Epoch: 1, Training Loss: 0.85, Validation Loss: 0.64, Accuracy: 85.28\n",
      "Epoch: 2, Training Loss: 0.81, Validation Loss: 0.61, Accuracy: 85.28\n",
      "Epoch: 3, Training Loss: 0.77, Validation Loss: 0.59, Accuracy: 84.17\n",
      "Epoch: 4, Training Loss: 0.73, Validation Loss: 0.58, Accuracy: 84.44\n",
      "Epoch: 5, Training Loss: 0.69, Validation Loss: 0.57, Accuracy: 84.17\n",
      "Epoch: 6, Training Loss: 0.65, Validation Loss: 0.55, Accuracy: 83.33\n",
      "Epoch: 7, Training Loss: 0.62, Validation Loss: 0.54, Accuracy: 83.06\n",
      "Epoch: 8, Training Loss: 0.59, Validation Loss: 0.52, Accuracy: 84.17\n",
      "Epoch: 9, Training Loss: 0.56, Validation Loss: 0.50, Accuracy: 83.89\n",
      "Epoch: 10, Training Loss: 0.54, Validation Loss: 0.48, Accuracy: 84.44\n",
      "Epoch: 11, Training Loss: 0.52, Validation Loss: 0.50, Accuracy: 82.22\n",
      "Epoch: 12, Training Loss: 0.50, Validation Loss: 0.44, Accuracy: 85.56\n",
      "Epoch: 13, Training Loss: 0.48, Validation Loss: 0.42, Accuracy: 86.39\n",
      "Epoch: 14, Training Loss: 0.46, Validation Loss: 0.38, Accuracy: 88.61\n",
      "Epoch: 15, Training Loss: 0.44, Validation Loss: 0.36, Accuracy: 89.72\n",
      "Epoch: 16, Training Loss: 0.41, Validation Loss: 0.33, Accuracy: 90.56\n",
      "Epoch: 17, Training Loss: 0.39, Validation Loss: 0.32, Accuracy: 90.83\n",
      "Epoch: 18, Training Loss: 0.37, Validation Loss: 0.30, Accuracy: 91.39\n",
      "Epoch: 19, Training Loss: 0.36, Validation Loss: 0.28, Accuracy: 92.22\n"
     ]
    }
   ],
   "source": [
    "train(cnn1, optimizer1, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66ccc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.34, Validation Loss: 0.26, Accuracy: 92.78\n",
      "Epoch: 1, Training Loss: 0.32, Validation Loss: 0.25, Accuracy: 93.89\n",
      "Epoch: 2, Training Loss: 0.31, Validation Loss: 0.24, Accuracy: 94.17\n",
      "Epoch: 3, Training Loss: 0.30, Validation Loss: 0.23, Accuracy: 94.44\n",
      "Epoch: 4, Training Loss: 0.29, Validation Loss: 0.22, Accuracy: 94.72\n",
      "Epoch: 5, Training Loss: 0.28, Validation Loss: 0.21, Accuracy: 95.00\n",
      "Epoch: 6, Training Loss: 0.27, Validation Loss: 0.21, Accuracy: 94.17\n",
      "Epoch: 7, Training Loss: 0.27, Validation Loss: 0.19, Accuracy: 95.00\n",
      "Epoch: 8, Training Loss: 0.25, Validation Loss: 0.19, Accuracy: 94.72\n",
      "Epoch: 9, Training Loss: 0.25, Validation Loss: 0.17, Accuracy: 94.72\n",
      "Epoch: 10, Training Loss: 0.23, Validation Loss: 0.17, Accuracy: 94.72\n",
      "Epoch: 11, Training Loss: 0.24, Validation Loss: 0.16, Accuracy: 94.72\n",
      "Epoch: 12, Training Loss: 0.22, Validation Loss: 0.15, Accuracy: 95.56\n",
      "Epoch: 13, Training Loss: 0.22, Validation Loss: 0.14, Accuracy: 96.11\n",
      "Epoch: 14, Training Loss: 0.20, Validation Loss: 0.14, Accuracy: 96.11\n",
      "Epoch: 15, Training Loss: 0.21, Validation Loss: 0.13, Accuracy: 96.11\n",
      "Epoch: 16, Training Loss: 0.18, Validation Loss: 0.13, Accuracy: 96.39\n",
      "Epoch: 17, Training Loss: 0.19, Validation Loss: 0.12, Accuracy: 96.67\n",
      "Epoch: 18, Training Loss: 0.17, Validation Loss: 0.13, Accuracy: 97.22\n",
      "Epoch: 19, Training Loss: 0.18, Validation Loss: 0.13, Accuracy: 97.22\n"
     ]
    }
   ],
   "source": [
    "train(cnn1, optimizer1, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39ad9575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.16, Validation Loss: 0.13, Accuracy: 96.67\n",
      "Epoch: 1, Training Loss: 0.17, Validation Loss: 0.14, Accuracy: 96.67\n",
      "Epoch: 2, Training Loss: 0.16, Validation Loss: 0.15, Accuracy: 96.67\n",
      "Epoch: 3, Training Loss: 0.16, Validation Loss: 0.16, Accuracy: 95.83\n",
      "Epoch: 4, Training Loss: 0.16, Validation Loss: 0.14, Accuracy: 96.67\n",
      "Epoch: 5, Training Loss: 0.15, Validation Loss: 0.15, Accuracy: 95.83\n",
      "Epoch: 6, Training Loss: 0.15, Validation Loss: 0.15, Accuracy: 95.83\n",
      "Epoch: 7, Training Loss: 0.15, Validation Loss: 0.15, Accuracy: 95.28\n",
      "Epoch: 8, Training Loss: 0.15, Validation Loss: 0.15, Accuracy: 95.83\n",
      "Epoch: 9, Training Loss: 0.14, Validation Loss: 0.14, Accuracy: 95.83\n",
      "Epoch: 10, Training Loss: 0.15, Validation Loss: 0.14, Accuracy: 96.39\n",
      "Epoch: 11, Training Loss: 0.13, Validation Loss: 0.15, Accuracy: 95.28\n",
      "Epoch: 12, Training Loss: 0.14, Validation Loss: 0.15, Accuracy: 95.28\n",
      "Epoch: 13, Training Loss: 0.13, Validation Loss: 0.15, Accuracy: 95.00\n",
      "Epoch: 14, Training Loss: 0.14, Validation Loss: 0.14, Accuracy: 95.83\n",
      "Epoch: 15, Training Loss: 0.13, Validation Loss: 0.14, Accuracy: 95.28\n",
      "Epoch: 16, Training Loss: 0.13, Validation Loss: 0.16, Accuracy: 95.00\n",
      "Epoch: 17, Training Loss: 0.12, Validation Loss: 0.15, Accuracy: 95.28\n",
      "Epoch: 18, Training Loss: 0.13, Validation Loss: 0.16, Accuracy: 94.72\n",
      "Epoch: 19, Training Loss: 0.11, Validation Loss: 0.16, Accuracy: 94.17\n"
     ]
    }
   ],
   "source": [
    "train(cnn1, optimizer1, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaba83",
   "metadata": {},
   "source": [
    "After 60 epochs, model start to overfit training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8df1d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.04, Validation Loss: 1.76, Accuracy: 16.67\n",
      "Epoch: 1, Training Loss: 1.79, Validation Loss: 1.72, Accuracy: 26.67\n",
      "Epoch: 2, Training Loss: 1.75, Validation Loss: 1.65, Accuracy: 32.78\n",
      "Epoch: 3, Training Loss: 1.75, Validation Loss: 1.58, Accuracy: 32.22\n",
      "Epoch: 4, Training Loss: 1.75, Validation Loss: 1.58, Accuracy: 32.22\n",
      "Epoch: 5, Training Loss: 1.74, Validation Loss: 1.54, Accuracy: 30.00\n",
      "Epoch: 6, Training Loss: 1.70, Validation Loss: 1.50, Accuracy: 44.44\n",
      "Epoch: 7, Training Loss: 1.68, Validation Loss: 1.44, Accuracy: 47.78\n",
      "Epoch: 8, Training Loss: 1.64, Validation Loss: 1.39, Accuracy: 50.56\n",
      "Epoch: 9, Training Loss: 1.61, Validation Loss: 1.35, Accuracy: 52.50\n",
      "Epoch: 10, Training Loss: 1.58, Validation Loss: 1.30, Accuracy: 57.22\n",
      "Epoch: 11, Training Loss: 1.54, Validation Loss: 1.24, Accuracy: 59.17\n",
      "Epoch: 12, Training Loss: 1.48, Validation Loss: 1.19, Accuracy: 61.11\n",
      "Epoch: 13, Training Loss: 1.44, Validation Loss: 1.14, Accuracy: 62.50\n",
      "Epoch: 14, Training Loss: 1.40, Validation Loss: 1.08, Accuracy: 61.94\n",
      "Epoch: 15, Training Loss: 1.37, Validation Loss: 1.01, Accuracy: 67.78\n",
      "Epoch: 16, Training Loss: 1.29, Validation Loss: 0.95, Accuracy: 70.00\n",
      "Epoch: 17, Training Loss: 1.19, Validation Loss: 0.89, Accuracy: 73.06\n",
      "Epoch: 18, Training Loss: 1.10, Validation Loss: 0.87, Accuracy: 72.78\n",
      "Epoch: 19, Training Loss: 1.03, Validation Loss: 0.84, Accuracy: 66.67\n",
      "Epoch: 20, Training Loss: 0.98, Validation Loss: 0.83, Accuracy: 65.00\n",
      "Epoch: 21, Training Loss: 0.94, Validation Loss: 0.80, Accuracy: 65.56\n",
      "Epoch: 22, Training Loss: 0.91, Validation Loss: 0.80, Accuracy: 65.28\n",
      "Epoch: 23, Training Loss: 0.91, Validation Loss: 0.83, Accuracy: 65.56\n",
      "Epoch: 24, Training Loss: 1.04, Validation Loss: 0.94, Accuracy: 69.72\n",
      "Epoch: 25, Training Loss: 1.05, Validation Loss: 0.78, Accuracy: 76.39\n",
      "Epoch: 26, Training Loss: 0.95, Validation Loss: 0.69, Accuracy: 81.67\n",
      "Epoch: 27, Training Loss: 0.89, Validation Loss: 0.63, Accuracy: 83.33\n",
      "Epoch: 28, Training Loss: 0.83, Validation Loss: 0.58, Accuracy: 85.00\n",
      "Epoch: 29, Training Loss: 0.78, Validation Loss: 0.54, Accuracy: 86.67\n",
      "Epoch: 30, Training Loss: 0.73, Validation Loss: 0.50, Accuracy: 87.78\n",
      "Epoch: 31, Training Loss: 0.68, Validation Loss: 0.47, Accuracy: 88.89\n",
      "Epoch: 32, Training Loss: 0.64, Validation Loss: 0.44, Accuracy: 89.72\n",
      "Epoch: 33, Training Loss: 0.60, Validation Loss: 0.42, Accuracy: 90.00\n",
      "Epoch: 34, Training Loss: 0.57, Validation Loss: 0.39, Accuracy: 90.28\n",
      "Epoch: 35, Training Loss: 0.54, Validation Loss: 0.37, Accuracy: 91.11\n",
      "Epoch: 36, Training Loss: 0.51, Validation Loss: 0.34, Accuracy: 91.39\n",
      "Epoch: 37, Training Loss: 0.49, Validation Loss: 0.33, Accuracy: 91.39\n",
      "Epoch: 38, Training Loss: 0.45, Validation Loss: 0.31, Accuracy: 92.22\n",
      "Epoch: 39, Training Loss: 0.43, Validation Loss: 0.29, Accuracy: 92.78\n",
      "Epoch: 40, Training Loss: 0.41, Validation Loss: 0.28, Accuracy: 93.33\n",
      "Epoch: 41, Training Loss: 0.40, Validation Loss: 0.27, Accuracy: 93.61\n",
      "Epoch: 42, Training Loss: 0.38, Validation Loss: 0.25, Accuracy: 93.89\n",
      "Epoch: 43, Training Loss: 0.36, Validation Loss: 0.24, Accuracy: 94.72\n",
      "Epoch: 44, Training Loss: 0.35, Validation Loss: 0.22, Accuracy: 94.72\n",
      "Epoch: 45, Training Loss: 0.33, Validation Loss: 0.21, Accuracy: 94.72\n",
      "Epoch: 46, Training Loss: 0.32, Validation Loss: 0.20, Accuracy: 94.72\n",
      "Epoch: 47, Training Loss: 0.30, Validation Loss: 0.19, Accuracy: 95.28\n",
      "Epoch: 48, Training Loss: 0.29, Validation Loss: 0.19, Accuracy: 95.56\n",
      "Epoch: 49, Training Loss: 0.27, Validation Loss: 0.18, Accuracy: 95.83\n",
      "Epoch: 50, Training Loss: 0.26, Validation Loss: 0.18, Accuracy: 96.39\n",
      "Epoch: 51, Training Loss: 0.24, Validation Loss: 0.17, Accuracy: 96.11\n",
      "Epoch: 52, Training Loss: 0.23, Validation Loss: 0.17, Accuracy: 96.11\n",
      "Epoch: 53, Training Loss: 0.21, Validation Loss: 0.16, Accuracy: 96.11\n",
      "Epoch: 54, Training Loss: 0.21, Validation Loss: 0.16, Accuracy: 96.11\n",
      "Epoch: 55, Training Loss: 0.20, Validation Loss: 0.16, Accuracy: 96.11\n",
      "Epoch: 56, Training Loss: 0.19, Validation Loss: 0.16, Accuracy: 96.11\n",
      "Epoch: 57, Training Loss: 0.18, Validation Loss: 0.16, Accuracy: 96.67\n",
      "Epoch: 58, Training Loss: 0.18, Validation Loss: 0.16, Accuracy: 96.67\n",
      "Epoch: 59, Training Loss: 0.17, Validation Loss: 0.16, Accuracy: 96.39\n"
     ]
    }
   ],
   "source": [
    "cnn2 = CNNNet()\n",
    "optimizer2 = optim.Adam(cnn2.parameters(), lr=5e-5)\n",
    "train(cnn2, optimizer2, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfa4ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn2, r'..\\cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d17ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = torch.load(r'..\\cnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86fc374",
   "metadata": {},
   "source": [
    "<h2>Final Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe145a6",
   "metadata": {},
   "source": [
    "Let's see our model predicting some images it never saw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scracthes_test_sample = r'..\\test\\scratches\\scratches_10.jpg'\n",
    "# patches_test_sample = r'..\\test\\patches\\patches_10.jpg'\n",
    "# inclusion_test_sample = r'..\\test\\inclusion\\inclusion_10.jpg'\n",
    "\n",
    "# def image_and_pred(model, img_dir):\n",
    "#     img = Image.open(img_dir)\n",
    "#     img = transforms(img)\n",
    "#     img = img.to(device)\n",
    "#     img = img.unsqueeze(0)\n",
    "#     prediction = model(img) # saved model is on gpu, no need to model.to(device)\n",
    "#     prediction = prediction.argmax()\n",
    "#     print('Predicted label: ', categories[prediction])\n",
    "#     process(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88145f99",
   "metadata": {},
   "source": [
    "And here is the performance on whole test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9038d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.15, Avg accuracy: 97.68\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "acc = list()\n",
    "for inputs, targets in test_data_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    output = cnn(inputs)\n",
    "    targets = targets.to(device)\n",
    "    batch_loss = torch.nn.CrossEntropyLoss()(output, targets)\n",
    "    test_loss += batch_loss.data.item() # cumulatively adding losses on batches\n",
    "    _, y_pred_tags = output.max(1)\n",
    "    total += targets.size(0)\n",
    "    correct += y_pred_tags.eq(targets).sum().item()\n",
    "    acc.append(100.*correct/total) # accuracy on a batch\n",
    "\n",
    "test_loss /= len(test_data_loader) # total test loss\n",
    "avg_acc = np.mean(acc)\n",
    "\n",
    "print('Test loss: {:.2f}, Avg accuracy: {:.2f}'.format(test_loss, avg_acc)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
